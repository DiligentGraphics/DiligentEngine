!!python/object/new:pybullet_envs.minitaur.agents.tools.attr_dict.AttrDict
dictitems:
  algorithm: !!python/name:pybullet_envs.minitaur.agents.ppo.algorithm.PPOAlgorithm ''
  discount: 0.9899764168788918
  env: !!python/object/apply:functools.partial
    args:
    - &id001 !!python/name:pybullet_envs.minitaur.envs.minitaur_trotting_env.MinitaurTrottingEnv ''
    state: !!python/tuple
    - *id001
    - !!python/tuple []
    - env_randomizer: null
      motor_kd: 0.015
      num_steps_to_log: 1000
      pd_latency: 0.003
      remove_default_joint_damping: true
      render: false
      urdf_version: rainbow_dash_v0
    - null
  eval_episodes: 25
  init_logstd: -0.6325707791047228
  init_mean_factor: 0.6508531688665261
  kl_cutoff_coef: 1000
  kl_cutoff_factor: 2
  kl_init_penalty: 1
  kl_target: 0.01
  logdir: /cns/ij-d/home/jietan/experiment/minitaur_vizier_study_ppo/mintrot_nonexp_nr_01_186515603_186518344/373
  max_length: 1000
  network: !!python/name:pybullet_envs.minitaur.agents.scripts.networks.ForwardGaussianPolicy ''
  network_config: {}
  num_agents: 25
  policy_layers: !!python/tuple
  - 133
  - 100
  policy_lr: 0.00048104185841752015
  policy_optimizer: AdamOptimizer
  steps: 7000000.0
  update_epochs_policy: 25
  update_epochs_value: 25
  update_every: 25
  use_gpu: false
  value_layers: !!python/tuple
  - 64
  - 57
  value_lr: 0.0012786382882055453
  value_optimizer: AdamOptimizer
  weight_summaries:
    all: .*
    policy: .*/policy/.*
    value: .*/value/.*
state:
  _mutable: false
